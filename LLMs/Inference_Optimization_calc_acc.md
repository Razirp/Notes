# 大模型推理加速技术——计算加速

> 本文综合了互联网上若干篇技术博客的内容，仅供个人学习使用，参考文献已于文后列出，若有侵权请联系我进行删除。

## 总览

计算加速：

- 通过**改进算法**和**硬件利用率**来提高效率
- 不影响模型输出的质量
- 让模型“算得更快”

主要方法：

- 算子融合
- KV Cache
- Kernel 优化
- 分布式推理

## 瓶颈

### Attention

对于GPT系列等主流的Decoder-only大模型来说，推理过程分为两个阶段：

- Context Phase/Prefill Phase 上下文阶段/预填充阶段
  - 计算整个Prompt的**自注意力**，
  - Q、K、V大小皆为`[seq_len, emb_dim]`
  - 只需要进行一次，生成第一个Token
- Genneration Phase/Decoding Phase 生成阶段/解码阶段
  - 每生成一个token就要计算一次，
  - Q大小为`[1, emb_dim]`，
  - K、V大小为`[n, emb_dim]`，代表所有前文的embedding
  - 计算的是**当前token**和**所有前文**的注意力

### 计算瓶颈

- 大模型通常需要处理很长的输入输出，输入越长，模型需要计算的矩阵尺寸越大
- 生成阶段，每个token的生成都依赖于前面所有的计算，只能一个个token生成，不能并行计算

即，Attention的计算是一个**$O(n^{2})$​**的操作，计算**时间复杂度高**，并且**无法并发**。

### 优化目标

- 更低的**延迟（Latency）**
  - 单个请求返回的时间
- 更高的**吞吐量（Throughput）**
  - 一定时间内处理请求的总量

#### 技术分类

- 计算加速
  - 通过改进算法和硬件利用率来提高效率
  - 不影响模型的输出质量
  - 让模型“算得更快”
- 模型压缩
  - 改变模型结构，减少部分计算（如稀疏Attention）或降低计算精度（量化），换来更快的推理速度和更低的资源消耗
  - 但可能会影响输出质量
  - 让模型“算得更少”

## 计算加速

在模型结构固定的情况下，可做的优化如以下三种：

- 减少需要执行的指令数量 - 计算侧优化
  - 减少不必要或重复的计算
- 充分利用硬件的并发度 - 计算侧优化
  - SIMD
  - 利用多核机制，同时执行多条指令
- 加速内存I/O速度 - 内存I/O优化
  - 利用缓存局部性加速内存读取指令的执行速度
  - 减少不必要的内存读写

### 计算侧优化

#### KV Cache

#### Kernel优化和算子融合

#### 分布式推理

#### 内存I/O优化

#### Flash Attention

在进行Attention计算时，Q、K、V都是非常大的矩阵，直接进行矩阵乘法运算非常缓存不友好；

可以考虑对矩阵进行分块乘法，每次只计算一个小的block，以保证block可以放进SRAM（一级缓存）而非HBM（二级缓存）中。 - 大部分矩阵乘法kernel的经典实现思路

Flash Attention更进一步。观察到Attention计算分为三步：

1. 

## 参考文献

- [大语言模型推理加速技术：计算加速篇 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/666452391)
- 