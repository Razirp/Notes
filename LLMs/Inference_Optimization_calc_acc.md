# 大模型推理加速技术——计算加速

> 本文综合了互联网上若干篇技术博客的内容，仅供个人学习使用，参考文献已于文后列出，若有侵权请联系我进行删除。

## 总览

计算加速：

- 通过**改进算法**和**硬件利用率**来提高效率
- 不影响模型输出的质量
- 让模型“算得更快”

主要方法：

- 算子融合
- KV Cache
- Kernel 优化
- 分布式推理

## 瓶颈

### Attention

对于GPT系列等主流的Decoder-only大模型来说，推理过程分为两个阶段：

- Context Phase/Prefill Phase 上下文阶段/预填充阶段
  - 计算整个Prompt的**自注意力**，
  - Q、K、V大小皆为`[seq_len, emb_dim]`
  - 只需要进行一次，生成第一个Token
- Genneration Phase/Decoding Phase 生成阶段/解码阶段
  - 每生成一个token就要计算一次，
  - Q大小为`[1, emb_dim]`，
  - K、V大小为`[n, emb_dim]`，代表所有前文的embedding
  - 计算的是**当前token**和**所有前文**的注意力

### 计算瓶颈

- 大模型通常需要处理很长的输入输出，输入越长，模型需要计算的矩阵尺寸越大
- 生成阶段，每个token的生成都依赖于前面所有的计算，只能一个个token生成，不能并行计算

即，Attention的计算是一个**$O(n^{2})$​**的操作，计算**时间复杂度高**，并且**无法并发**。

### 优化目标

- 更低的**延迟（Latency）**
  - 单个请求返回的时间
- 更高的**吞吐量（Throughput）**
  - 一定时间内处理请求的总量

## 参考文献

- [大语言模型推理加速技术：计算加速篇 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/666452391)
- 